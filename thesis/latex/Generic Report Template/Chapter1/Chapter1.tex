% !TEX root =  ../Dissertation.tex

\chapter{Introduction}


\subsection{Background}Speech conveys rich affective information through prosody, articulation, and spectral characteristics, enabling listeners to infer speakers’ intentions and internal states. Speech Emotion Recognition (SER) aims to computationally infer human emotion from acoustic signals, either as discrete categories (e.g., anger, happiness, sadness, fear, and neutral) or along continuous dimensions (e.g., valence, arousal, and dominance). Rooted in speech processing, affective computing, and psychology, early SER systems relied on handcrafted prosodic and spectral features combined with classical classifiers. Over the past decade, deep learning has transformed the field: convolutional networks on time–frequency representations, recurrent and attention-based models for temporal dynamics, and, more recently, transformer architectures and self-supervised pretraining have markedly improved generalization. The research focus has also shifted from acted laboratory corpora to spontaneous, in-the-wild data, emphasizing robustness to noise, channel variability, and cultural–linguistic diversity. In parallel, there is growing attention to transparency, privacy, and fairness, reflecting the societal implications of emotion inference technologies.

SER has broad practical value. In customer service and contact centers, emotion-aware analytics can monitor user satisfaction and support agent coaching in real time. In healthcare and mental health, non-invasive acoustic markers assist in screening and longitudinal monitoring of affective disorders, stress, and burnout. In human–computer interaction, conversational assistants and social robots can adapt responses and dialogue strategies based on users’ emotional cues. In automotive and smart environments, continuous monitoring of driver frustration or fatigue enhances safety and personalization. Education technology, entertainment, and recommendation systems also benefit from affect-aware feedback and content curation. These applications motivate solutions that are accurate, low-latency, and resource-efficient for both cloud and on-device deployment.

The research significance of SER is twofold. Scientifically, SER advances our understanding of how affect is encoded in speech and how to learn robust, interpretable representations under limited, imbalanced, and noisy supervision. Technically, it drives innovation in domain generalization, cross-lingual transfer, and multimodal fusion with text and vision, while promoting explainability to build user trust. Addressing ethical and legal considerations—including data privacy, informed consent, and bias mitigation—is essential to responsible deployment. Finally, rigorous benchmarking, reproducibility, and standardized evaluation protocols (e.g., unweighted/weighted accuracy for categorical tasks and concordance correlation for continuous estimation) are critical for cumulative progress. Altogether, SER research provides both theoretical insights and practical tools for empathetic, human-centered intelligent systems.


\subsection{Main Contents of the Dissertation}
This dissertation addresses the challenge of suboptimal recognition accuracy in Speech Emotion Recognition by developing a deep learning based framework that integrates speech signal preprocessing, feature engineering, attention mechanisms, and model optimization. We conduct a systematic study of SER methods from multiple perspectives, including audio preprocessing, data augmentation, feature selection, model architectures, and evaluation metrics. The central objective is to learn feature representations with stronger affective discriminability directly from speech, thereby improving accuracy, robustness, and generalization across speakers, recording conditions, and datasets. The proposed approach emphasizes practical deployability by balancing performance with computational efficiency for both cloud and on-device inference.

\subsection{Structure of the Dissertation}
The dissertation is organized as follows.
Chapter 1 introduces the research background and motivation, identifies the key challenges in SER, and outlines the scope and structure of the thesis.
Chapter 2 surveys related work on speech signal processing, feature extraction, the design of attention mechanisms, and emotion taxonomies and recognition paradigms, summarizing major international developments.
Chapter 3 presents the methodology. We construct features from audio data using Mel-frequency cepstral coefficients (MFCCs) and Mel-spectrograms; we then investigate multiple models for SER, including a multilayer perceptron (MLP), a shallow convolutional neural network (CNN), ResNet-18 for 2D Mel-spectrogram inputs, and the Audio Spectrogram Transformer (AST), a ViT-based model for audio classification. Finally, we apply data augmentation by injecting noise during training to improve generalization and mitigate overfitting.
Chapter 4 describes the experiments. We design comparative studies to evaluate the performance differences among features and models on SER tasks under consistent protocols and metrics.
Chapter 5 provides discussion. We analyze the experimental results in depth, examine the relative strengths and weaknesses of different features and models, and articulate limitations and directions for future research.
Chapter 6 concludes the thesis by summarizing the main findings and contributions, and highlighting potential avenues for subsequent work.
